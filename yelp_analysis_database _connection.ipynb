{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 3316532,
          "sourceType": "datasetVersion",
          "datasetId": 10100
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import kagglehub\n",
        "organizations_yelp_dataset_yelp_dataset_path = kagglehub.dataset_download('yelp-dataset/yelp-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "xHlb-HbDJFP1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c96c3e4-8b34-4e41-96f1-a07c51fcab76"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/yelp-dataset/yelp-dataset?dataset_version_number=4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.07G/4.07G [01:11<00:00, 61.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-11-04T02:20:24.704555Z",
          "iopub.execute_input": "2024-11-04T02:20:24.705244Z",
          "iopub.status.idle": "2024-11-04T02:20:24.717343Z",
          "shell.execute_reply.started": "2024-11-04T02:20:24.705182Z",
          "shell.execute_reply": "2024-11-04T02:20:24.715816Z"
        },
        "trusted": true,
        "id": "TsD1u5BnJFP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extraction"
      ],
      "metadata": {
        "id": "NaQ2w6jWJFP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define file paths\n",
        "file_paths = {\n",
        "    'business': '/kaggle/input/yelp-dataset/yelp_academic_dataset_business.json',\n",
        "    'checkin': '/kaggle/input/yelp-dataset/yelp_academic_dataset_checkin.json',\n",
        "    'review': '/kaggle/input/yelp-dataset/yelp_academic_dataset_review.json',\n",
        "    'tip': '/kaggle/input/yelp-dataset/yelp_academic_dataset_tip.json',\n",
        "    'user': '/kaggle/input/yelp-dataset/yelp_academic_dataset_user.json'\n",
        "}\n",
        "\n",
        "# Define an empty dictionary to store DataFrames\n",
        "dataframes = {}\n",
        "\n",
        "# Define a function to read JSON in chunks and concatenate\n",
        "def load_json_in_chunks(file_path, chunksize=10000):\n",
        "    # Create an iterator for the JSON file\n",
        "    json_reader = pd.read_json(file_path, lines=True, chunksize=chunksize)\n",
        "    # Concatenate chunks into a single DataFrame\n",
        "    return pd.concat([chunk for chunk in json_reader], ignore_index=True)\n",
        "\n",
        "# Load each file in chunks and store it in the dictionary\n",
        "for name, path in file_paths.items():\n",
        "    print(f\"Loading {name} data in chunks...\")\n",
        "    dataframes[name] = load_json_in_chunks(path)\n",
        "    print(f\"{name.capitalize()} Data Loaded. Shape: {dataframes[name].shape}\")\n",
        "\n",
        "# Access each DataFrame\n",
        "business_df = dataframes['business']\n",
        "checkin_df = dataframes['checkin']\n",
        "review_df = dataframes['review']\n",
        "tip_df = dataframes['tip']\n",
        "user_df = dataframes['user']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-04T02:20:32.835464Z",
          "iopub.execute_input": "2024-11-04T02:20:32.835982Z",
          "iopub.status.idle": "2024-11-04T02:24:19.01061Z",
          "shell.execute_reply.started": "2024-11-04T02:20:32.835935Z",
          "shell.execute_reply": "2024-11-04T02:24:19.009159Z"
        },
        "trusted": true,
        "id": "u8v013jvJFP9",
        "outputId": "6a08e098-a407-4fb3-f423-ad41684e59fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading business data in chunks...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "File /kaggle/input/yelp-dataset/yelp_academic_dataset_business.json does not exist",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-2f2072c2766d>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading {name} data in chunks...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mdataframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_json_in_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{name.capitalize()} Data Loaded. Shape: {dataframes[name].shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-2f2072c2766d>\u001b[0m in \u001b[0;36mload_json_in_chunks\u001b[0;34m(file_path, chunksize)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_json_in_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Create an iterator for the JSON file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mjson_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Concatenate chunks into a single DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchunk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mconvert_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m     json_reader = JsonReader(\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ujson\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data_from_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_data_from_filepath\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m         ):\n\u001b[0;32m--> 960\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"File {filepath_or_buffer} does not exist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: File /kaggle/input/yelp-dataset/yelp_academic_dataset_business.json does not exist"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Analysis"
      ],
      "metadata": {
        "id": "f-WXa7H1JFP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "business_df.info()\n",
        "checkin_df.info()\n",
        "review_df.info()\n",
        "tip_df.info()\n",
        "user_df.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-04T02:24:28.837921Z",
          "iopub.execute_input": "2024-11-04T02:24:28.839632Z",
          "iopub.status.idle": "2024-11-04T02:24:29.677203Z",
          "shell.execute_reply.started": "2024-11-04T02:24:28.839576Z",
          "shell.execute_reply": "2024-11-04T02:24:29.675583Z"
        },
        "trusted": true,
        "id": "XRhpkTJjJFQA",
        "outputId": "d12a7d09-6f8b-4c66-a0c7-566244d456c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 150346 entries, 0 to 150345\nData columns (total 14 columns):\n #   Column        Non-Null Count   Dtype  \n---  ------        --------------   -----  \n 0   business_id   150346 non-null  object \n 1   name          150346 non-null  object \n 2   address       150346 non-null  object \n 3   city          150346 non-null  object \n 4   state         150346 non-null  object \n 5   postal_code   150346 non-null  object \n 6   latitude      150346 non-null  float64\n 7   longitude     150346 non-null  float64\n 8   stars         150346 non-null  float64\n 9   review_count  150346 non-null  int64  \n 10  is_open       150346 non-null  int64  \n 11  attributes    136602 non-null  object \n 12  categories    150243 non-null  object \n 13  hours         127123 non-null  object \ndtypes: float64(3), int64(2), object(9)\nmemory usage: 16.1+ MB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 131930 entries, 0 to 131929\nData columns (total 2 columns):\n #   Column       Non-Null Count   Dtype \n---  ------       --------------   ----- \n 0   business_id  131930 non-null  object\n 1   date         131930 non-null  object\ndtypes: object(2)\nmemory usage: 2.0+ MB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 6990280 entries, 0 to 6990279\nData columns (total 9 columns):\n #   Column       Dtype         \n---  ------       -----         \n 0   review_id    object        \n 1   user_id      object        \n 2   business_id  object        \n 3   stars        int64         \n 4   useful       int64         \n 5   funny        int64         \n 6   cool         int64         \n 7   text         object        \n 8   date         datetime64[ns]\ndtypes: datetime64[ns](1), int64(4), object(4)\nmemory usage: 480.0+ MB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 908915 entries, 0 to 908914\nData columns (total 5 columns):\n #   Column            Non-Null Count   Dtype         \n---  ------            --------------   -----         \n 0   user_id           908915 non-null  object        \n 1   business_id       908915 non-null  object        \n 2   text              908915 non-null  object        \n 3   date              908915 non-null  datetime64[ns]\n 4   compliment_count  908915 non-null  int64         \ndtypes: datetime64[ns](1), int64(1), object(3)\nmemory usage: 34.7+ MB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1987897 entries, 0 to 1987896\nData columns (total 22 columns):\n #   Column              Dtype  \n---  ------              -----  \n 0   user_id             object \n 1   name                object \n 2   review_count        int64  \n 3   yelping_since       object \n 4   useful              int64  \n 5   funny               int64  \n 6   cool                int64  \n 7   elite               object \n 8   friends             object \n 9   fans                int64  \n 10  average_stars       float64\n 11  compliment_hot      int64  \n 12  compliment_more     int64  \n 13  compliment_profile  int64  \n 14  compliment_cute     int64  \n 15  compliment_list     int64  \n 16  compliment_note     int64  \n 17  compliment_plain    int64  \n 18  compliment_cool     int64  \n 19  compliment_funny    int64  \n 20  compliment_writer   int64  \n 21  compliment_photos   int64  \ndtypes: float64(1), int64(16), object(5)\nmemory usage: 333.7+ MB\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display first few rows of each dataset to verify\n",
        "print(\"Business Data Sample:\")\n",
        "display(business_df.head())\n",
        "\n",
        "print(\"\\nCheck-in Data Sample:\")\n",
        "display(checkin_df.head())\n",
        "\n",
        "print(\"\\nReview Data Sample:\")\n",
        "display(review_df.head())\n",
        "\n",
        "print(\"\\nTip Data Sample:\")\n",
        "display(tip_df.head())\n",
        "\n",
        "print(\"\\nUser Data Sample:\")\n",
        "display(user_df.head())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-04T02:24:34.246593Z",
          "iopub.execute_input": "2024-11-04T02:24:34.247125Z",
          "iopub.status.idle": "2024-11-04T02:24:34.321627Z",
          "shell.execute_reply.started": "2024-11-04T02:24:34.247077Z",
          "shell.execute_reply": "2024-11-04T02:24:34.320475Z"
        },
        "trusted": true,
        "id": "aHFoQWmJJFQB",
        "outputId": "3fee6d63-5af9-4ee3-fc08-347f504c39aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Business Data Sample:\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "              business_id                      name  \\\n0  Pns2l4eNsfO8kk83dixA6A  Abby Rappoport, LAC, CMQ   \n1  mpf3x-BjTdTEA3yCZrAYPw             The UPS Store   \n2  tUFrWirKiKi_TAnsVWINQQ                    Target   \n3  MTSW4McQd7CbVtyjqoe9mw        St Honore Pastries   \n4  mWMc6_wTdE0EUBKIGXDVfA  Perkiomen Valley Brewery   \n\n                           address           city state postal_code  \\\n0           1616 Chapala St, Ste 2  Santa Barbara    CA       93101   \n1  87 Grasso Plaza Shopping Center         Affton    MO       63123   \n2             5255 E Broadway Blvd         Tucson    AZ       85711   \n3                      935 Race St   Philadelphia    PA       19107   \n4                    101 Walnut St     Green Lane    PA       18054   \n\n    latitude   longitude  stars  review_count  is_open  \\\n0  34.426679 -119.711197    5.0             7        0   \n1  38.551126  -90.335695    3.0            15        1   \n2  32.223236 -110.880452    3.5            22        0   \n3  39.955505  -75.155564    4.0            80        1   \n4  40.338183  -75.471659    4.5            13        1   \n\n                                          attributes  \\\n0                      {'ByAppointmentOnly': 'True'}   \n1             {'BusinessAcceptsCreditCards': 'True'}   \n2  {'BikeParking': 'True', 'BusinessAcceptsCredit...   \n3  {'RestaurantsDelivery': 'False', 'OutdoorSeati...   \n4  {'BusinessAcceptsCreditCards': 'True', 'Wheelc...   \n\n                                          categories  \\\n0  Doctors, Traditional Chinese Medicine, Naturop...   \n1  Shipping Centers, Local Services, Notaries, Ma...   \n2  Department Stores, Shopping, Fashion, Home & G...   \n3  Restaurants, Food, Bubble Tea, Coffee & Tea, B...   \n4                          Brewpubs, Breweries, Food   \n\n                                               hours  \n0                                               None  \n1  {'Monday': '0:0-0:0', 'Tuesday': '8:0-18:30', ...  \n2  {'Monday': '8:0-22:0', 'Tuesday': '8:0-22:0', ...  \n3  {'Monday': '7:0-20:0', 'Tuesday': '7:0-20:0', ...  \n4  {'Wednesday': '14:0-22:0', 'Thursday': '16:0-2...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>business_id</th>\n      <th>name</th>\n      <th>address</th>\n      <th>city</th>\n      <th>state</th>\n      <th>postal_code</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>stars</th>\n      <th>review_count</th>\n      <th>is_open</th>\n      <th>attributes</th>\n      <th>categories</th>\n      <th>hours</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Pns2l4eNsfO8kk83dixA6A</td>\n      <td>Abby Rappoport, LAC, CMQ</td>\n      <td>1616 Chapala St, Ste 2</td>\n      <td>Santa Barbara</td>\n      <td>CA</td>\n      <td>93101</td>\n      <td>34.426679</td>\n      <td>-119.711197</td>\n      <td>5.0</td>\n      <td>7</td>\n      <td>0</td>\n      <td>{'ByAppointmentOnly': 'True'}</td>\n      <td>Doctors, Traditional Chinese Medicine, Naturop...</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>mpf3x-BjTdTEA3yCZrAYPw</td>\n      <td>The UPS Store</td>\n      <td>87 Grasso Plaza Shopping Center</td>\n      <td>Affton</td>\n      <td>MO</td>\n      <td>63123</td>\n      <td>38.551126</td>\n      <td>-90.335695</td>\n      <td>3.0</td>\n      <td>15</td>\n      <td>1</td>\n      <td>{'BusinessAcceptsCreditCards': 'True'}</td>\n      <td>Shipping Centers, Local Services, Notaries, Ma...</td>\n      <td>{'Monday': '0:0-0:0', 'Tuesday': '8:0-18:30', ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>tUFrWirKiKi_TAnsVWINQQ</td>\n      <td>Target</td>\n      <td>5255 E Broadway Blvd</td>\n      <td>Tucson</td>\n      <td>AZ</td>\n      <td>85711</td>\n      <td>32.223236</td>\n      <td>-110.880452</td>\n      <td>3.5</td>\n      <td>22</td>\n      <td>0</td>\n      <td>{'BikeParking': 'True', 'BusinessAcceptsCredit...</td>\n      <td>Department Stores, Shopping, Fashion, Home &amp; G...</td>\n      <td>{'Monday': '8:0-22:0', 'Tuesday': '8:0-22:0', ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>MTSW4McQd7CbVtyjqoe9mw</td>\n      <td>St Honore Pastries</td>\n      <td>935 Race St</td>\n      <td>Philadelphia</td>\n      <td>PA</td>\n      <td>19107</td>\n      <td>39.955505</td>\n      <td>-75.155564</td>\n      <td>4.0</td>\n      <td>80</td>\n      <td>1</td>\n      <td>{'RestaurantsDelivery': 'False', 'OutdoorSeati...</td>\n      <td>Restaurants, Food, Bubble Tea, Coffee &amp; Tea, B...</td>\n      <td>{'Monday': '7:0-20:0', 'Tuesday': '7:0-20:0', ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>mWMc6_wTdE0EUBKIGXDVfA</td>\n      <td>Perkiomen Valley Brewery</td>\n      <td>101 Walnut St</td>\n      <td>Green Lane</td>\n      <td>PA</td>\n      <td>18054</td>\n      <td>40.338183</td>\n      <td>-75.471659</td>\n      <td>4.5</td>\n      <td>13</td>\n      <td>1</td>\n      <td>{'BusinessAcceptsCreditCards': 'True', 'Wheelc...</td>\n      <td>Brewpubs, Breweries, Food</td>\n      <td>{'Wednesday': '14:0-22:0', 'Thursday': '16:0-2...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\nCheck-in Data Sample:\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "              business_id                                               date\n0  ---kPU91CF4Lq2-WlRu9Lw  2020-03-13 21:10:56, 2020-06-02 22:18:06, 2020...\n1  --0iUa4sNDFiZFrAdIWhZQ  2010-09-13 21:43:09, 2011-05-04 23:08:15, 2011...\n2  --30_8IhuyMHbSOcNWd6DQ           2013-06-14 23:29:17, 2014-08-13 23:20:22\n3  --7PUidqRWpRSpXebiyxTg  2011-02-15 17:12:00, 2011-07-28 02:46:10, 2012...\n4  --7jw19RH9JKXgFohspgQw  2014-04-21 20:42:11, 2014-04-28 21:04:46, 2014...",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>business_id</th>\n      <th>date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>---kPU91CF4Lq2-WlRu9Lw</td>\n      <td>2020-03-13 21:10:56, 2020-06-02 22:18:06, 2020...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>--0iUa4sNDFiZFrAdIWhZQ</td>\n      <td>2010-09-13 21:43:09, 2011-05-04 23:08:15, 2011...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>--30_8IhuyMHbSOcNWd6DQ</td>\n      <td>2013-06-14 23:29:17, 2014-08-13 23:20:22</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>--7PUidqRWpRSpXebiyxTg</td>\n      <td>2011-02-15 17:12:00, 2011-07-28 02:46:10, 2012...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>--7jw19RH9JKXgFohspgQw</td>\n      <td>2014-04-21 20:42:11, 2014-04-28 21:04:46, 2014...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\nReview Data Sample:\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "                review_id                 user_id             business_id  \\\n0  KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA  XQfwVwDr-v0ZS3_CbbE5Xw   \n1  BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q  7ATYjTIgM3jUlt4UM3IypQ   \n2  saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A  YjUWPpI6HXG530lwP-fb2A   \n3  AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA   \n4  Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ  e4Vwtrqf-wpJfwesgvdgxQ   \n\n   stars  useful  funny  cool  \\\n0      3       0      0     0   \n1      5       1      0     1   \n2      3       0      0     0   \n3      5       1      0     1   \n4      4       1      0     1   \n\n                                                text                date  \n0  If you decide to eat here, just be aware it is... 2018-07-07 22:09:11  \n1  I've taken a lot of spin classes over the year... 2012-01-03 15:28:18  \n2  Family diner. Had the buffet. Eclectic assortm... 2014-02-05 20:30:30  \n3  Wow!  Yummy, different,  delicious.   Our favo... 2015-01-04 00:01:03  \n4  Cute interior and owner (?) gave us tour of up... 2017-01-14 20:54:15  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_id</th>\n      <th>user_id</th>\n      <th>business_id</th>\n      <th>stars</th>\n      <th>useful</th>\n      <th>funny</th>\n      <th>cool</th>\n      <th>text</th>\n      <th>date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>KU_O5udG6zpxOg-VcAEodg</td>\n      <td>mh_-eMZ6K5RLWhZyISBhwA</td>\n      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>If you decide to eat here, just be aware it is...</td>\n      <td>2018-07-07 22:09:11</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BiTunyQ73aT9WBnpR9DZGw</td>\n      <td>OyoGAe7OKpv6SyGZT5g77Q</td>\n      <td>7ATYjTIgM3jUlt4UM3IypQ</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>I've taken a lot of spin classes over the year...</td>\n      <td>2012-01-03 15:28:18</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>saUsX_uimxRlCVr67Z4Jig</td>\n      <td>8g_iMtfSiwikVnbP2etR0A</td>\n      <td>YjUWPpI6HXG530lwP-fb2A</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n      <td>2014-02-05 20:30:30</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AqPFMleE6RsU23_auESxiA</td>\n      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n      <td>2015-01-04 00:01:03</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sx8TMOWLNuJBWer-0pcmoA</td>\n      <td>bcjbaE6dDog4jkNY91ncLQ</td>\n      <td>e4Vwtrqf-wpJfwesgvdgxQ</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Cute interior and owner (?) gave us tour of up...</td>\n      <td>2017-01-14 20:54:15</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\nTip Data Sample:\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "                  user_id             business_id  \\\n0  AGNUgVwnZUey3gcPCJ76iw  3uLgwr0qeCNMjKenHJwPGQ   \n1  NBN4MgHP9D3cw--SnauTkA  QoezRbYQncpRqyrLH6Iqjg   \n2  -copOvldyKh1qr-vzkDEvw  MYoRNLb5chwjQe3c_k37Gg   \n3  FjMQVZjSqY8syIO-53KFKw  hV-bABTK-glh5wj31ps_Jw   \n4  ld0AperBXk1h6UbqmM80zw  _uN0OudeJ3Zl_tf6nxg5ww   \n\n                                                text                date  \\\n0                     Avengers time with the ladies. 2012-05-18 02:17:21   \n1  They have lots of good deserts and tasty cuban... 2013-02-05 18:35:10   \n2             It's open even when you think it isn't 2013-08-18 00:56:08   \n3                          Very decent fried chicken 2017-06-27 23:05:38   \n4             Appetizers.. platter special for lunch 2012-10-06 19:43:09   \n\n   compliment_count  \n0                 0  \n1                 0  \n2                 0  \n3                 0  \n4                 0  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>business_id</th>\n      <th>text</th>\n      <th>date</th>\n      <th>compliment_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AGNUgVwnZUey3gcPCJ76iw</td>\n      <td>3uLgwr0qeCNMjKenHJwPGQ</td>\n      <td>Avengers time with the ladies.</td>\n      <td>2012-05-18 02:17:21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NBN4MgHP9D3cw--SnauTkA</td>\n      <td>QoezRbYQncpRqyrLH6Iqjg</td>\n      <td>They have lots of good deserts and tasty cuban...</td>\n      <td>2013-02-05 18:35:10</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-copOvldyKh1qr-vzkDEvw</td>\n      <td>MYoRNLb5chwjQe3c_k37Gg</td>\n      <td>It's open even when you think it isn't</td>\n      <td>2013-08-18 00:56:08</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>FjMQVZjSqY8syIO-53KFKw</td>\n      <td>hV-bABTK-glh5wj31ps_Jw</td>\n      <td>Very decent fried chicken</td>\n      <td>2017-06-27 23:05:38</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ld0AperBXk1h6UbqmM80zw</td>\n      <td>_uN0OudeJ3Zl_tf6nxg5ww</td>\n      <td>Appetizers.. platter special for lunch</td>\n      <td>2012-10-06 19:43:09</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\nUser Data Sample:\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "                  user_id    name  review_count        yelping_since  useful  \\\n0  qVc8ODYU5SZjKXVBgXdI7w  Walker           585  2007-01-25 16:47:26    7217   \n1  j14WgRoU_-2ZE1aw1dXrJg  Daniel          4333  2009-01-25 04:35:42   43091   \n2  2WnXYQFK0hXEoTxPtV2zvg   Steph           665  2008-07-25 10:41:00    2086   \n3  SZDeASXq7o05mMNLshsdIA    Gwen           224  2005-11-29 04:38:33     512   \n4  hA5lMy-EnncsH4JoR-hFGQ   Karen            79  2007-01-05 19:40:59      29   \n\n   funny   cool                                              elite  \\\n0   1259   5994                                               2007   \n1  13066  27281  2009,2010,2011,2012,2013,2014,2015,2016,2017,2...   \n2   1010   1003                           2009,2010,2011,2012,2013   \n3    330    299                                     2009,2010,2011   \n4     15      7                                                      \n\n                                             friends  fans  ...  \\\n0  NSCy54eWehBJyZdG2iE84w, pe42u7DcCH2QmI81NX-8qA...   267  ...   \n1  ueRPE0CX75ePGMqOFVj6IQ, 52oH4DrRvzzl8wh5UXyU0A...  3138  ...   \n2  LuO3Bn4f3rlhyHIaNfTlnA, j9B4XdHUhDfTKVecyWQgyA...    52  ...   \n3  enx1vVPnfdNUdPho6PH_wg, 4wOcvMLtU6a9Lslggq74Vg...    28  ...   \n4  PBK4q9KEEBHhFvSXCUirIw, 3FWPpM7KU1gXeOM_ZbYMbA...     1  ...   \n\n   compliment_more  compliment_profile  compliment_cute  compliment_list  \\\n0               65                  55               56               18   \n1              264                 184              157              251   \n2               13                  10               17                3   \n3                4                   1                6                2   \n4                1                   0                0                0   \n\n   compliment_note  compliment_plain  compliment_cool  compliment_funny  \\\n0              232               844              467               467   \n1             1847              7054             3131              3131   \n2               66                96              119               119   \n3               12                16               26                26   \n4                1                 1                0                 0   \n\n   compliment_writer  compliment_photos  \n0                239                180  \n1               1521               1946  \n2                 35                 18  \n3                 10                  9  \n4                  0                  0  \n\n[5 rows x 22 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>name</th>\n      <th>review_count</th>\n      <th>yelping_since</th>\n      <th>useful</th>\n      <th>funny</th>\n      <th>cool</th>\n      <th>elite</th>\n      <th>friends</th>\n      <th>fans</th>\n      <th>...</th>\n      <th>compliment_more</th>\n      <th>compliment_profile</th>\n      <th>compliment_cute</th>\n      <th>compliment_list</th>\n      <th>compliment_note</th>\n      <th>compliment_plain</th>\n      <th>compliment_cool</th>\n      <th>compliment_funny</th>\n      <th>compliment_writer</th>\n      <th>compliment_photos</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>qVc8ODYU5SZjKXVBgXdI7w</td>\n      <td>Walker</td>\n      <td>585</td>\n      <td>2007-01-25 16:47:26</td>\n      <td>7217</td>\n      <td>1259</td>\n      <td>5994</td>\n      <td>2007</td>\n      <td>NSCy54eWehBJyZdG2iE84w, pe42u7DcCH2QmI81NX-8qA...</td>\n      <td>267</td>\n      <td>...</td>\n      <td>65</td>\n      <td>55</td>\n      <td>56</td>\n      <td>18</td>\n      <td>232</td>\n      <td>844</td>\n      <td>467</td>\n      <td>467</td>\n      <td>239</td>\n      <td>180</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>j14WgRoU_-2ZE1aw1dXrJg</td>\n      <td>Daniel</td>\n      <td>4333</td>\n      <td>2009-01-25 04:35:42</td>\n      <td>43091</td>\n      <td>13066</td>\n      <td>27281</td>\n      <td>2009,2010,2011,2012,2013,2014,2015,2016,2017,2...</td>\n      <td>ueRPE0CX75ePGMqOFVj6IQ, 52oH4DrRvzzl8wh5UXyU0A...</td>\n      <td>3138</td>\n      <td>...</td>\n      <td>264</td>\n      <td>184</td>\n      <td>157</td>\n      <td>251</td>\n      <td>1847</td>\n      <td>7054</td>\n      <td>3131</td>\n      <td>3131</td>\n      <td>1521</td>\n      <td>1946</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2WnXYQFK0hXEoTxPtV2zvg</td>\n      <td>Steph</td>\n      <td>665</td>\n      <td>2008-07-25 10:41:00</td>\n      <td>2086</td>\n      <td>1010</td>\n      <td>1003</td>\n      <td>2009,2010,2011,2012,2013</td>\n      <td>LuO3Bn4f3rlhyHIaNfTlnA, j9B4XdHUhDfTKVecyWQgyA...</td>\n      <td>52</td>\n      <td>...</td>\n      <td>13</td>\n      <td>10</td>\n      <td>17</td>\n      <td>3</td>\n      <td>66</td>\n      <td>96</td>\n      <td>119</td>\n      <td>119</td>\n      <td>35</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>SZDeASXq7o05mMNLshsdIA</td>\n      <td>Gwen</td>\n      <td>224</td>\n      <td>2005-11-29 04:38:33</td>\n      <td>512</td>\n      <td>330</td>\n      <td>299</td>\n      <td>2009,2010,2011</td>\n      <td>enx1vVPnfdNUdPho6PH_wg, 4wOcvMLtU6a9Lslggq74Vg...</td>\n      <td>28</td>\n      <td>...</td>\n      <td>4</td>\n      <td>1</td>\n      <td>6</td>\n      <td>2</td>\n      <td>12</td>\n      <td>16</td>\n      <td>26</td>\n      <td>26</td>\n      <td>10</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>hA5lMy-EnncsH4JoR-hFGQ</td>\n      <td>Karen</td>\n      <td>79</td>\n      <td>2007-01-05 19:40:59</td>\n      <td>29</td>\n      <td>15</td>\n      <td>7</td>\n      <td></td>\n      <td>PBK4q9KEEBHhFvSXCUirIw, 3FWPpM7KU1gXeOM_ZbYMbA...</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 22 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detection and normalisation of Dataframes"
      ],
      "metadata": {
        "id": "evM_iGvlJFQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pandas import json_normalize\n",
        "\n",
        "# Define a function to detect nested JSON/dictionaries or arrays in a DataFrame\n",
        "def detect_nested_columns(df):\n",
        "    \"\"\"\n",
        "    Detects columns with nested JSON/dictionaries or arrays in a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): The DataFrame to inspect.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of column names containing nested structures.\n",
        "    \"\"\"\n",
        "    nested_columns = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        # Check the first non-null value in the column\n",
        "        first_value = df[col].dropna().values[0] if not df[col].dropna().empty else None\n",
        "\n",
        "        if isinstance(first_value, dict) or isinstance(first_value, list):\n",
        "            nested_columns.append(col)\n",
        "            print(f\"Column '{col}' contains nested data.\")\n",
        "\n",
        "    if not nested_columns:\n",
        "        print(\"No nested columns found.\")\n",
        "\n",
        "    return nested_columns\n",
        "\n",
        "# Define a function to normalize (flatten) nested columns in a DataFrame\n",
        "def normalize_nested_columns(df, nested_columns):\n",
        "    \"\"\"\n",
        "    Flattens specified nested columns in a DataFrame using json_normalize.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): The original DataFrame.\n",
        "        nested_columns (list): List of columns to flatten.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with nested fields flattened.\n",
        "    \"\"\"\n",
        "    for col in nested_columns:\n",
        "        # Apply json_normalize to each nested column\n",
        "        flat_df = json_normalize(df[col].dropna())  # Flatten only non-null entries\n",
        "        flat_df.columns = [f\"{col}_{subcol}\" for subcol in flat_df.columns]  # Rename columns to avoid conflicts\n",
        "        df = pd.concat([df.drop(columns=[col]), flat_df.reindex(df.index)], axis=1)  # Concatenate flattened data\n",
        "\n",
        "    # Fill any resulting NaN values with 0 if needed\n",
        "    df = df.fillna(0)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply detection and normalization for each table\n",
        "dataframes = {\n",
        "    \"business_df\": business_df,\n",
        "    \"checkin_df\": checkin_df,\n",
        "    \"review_df\": review_df,\n",
        "    \"tip_df\": tip_df,\n",
        "    \"user_df\": user_df\n",
        "}\n",
        "\n",
        "# Process each DataFrame\n",
        "for name, df in dataframes.items():\n",
        "    print(f\"\\nProcessing {name}...\")\n",
        "\n",
        "    # Step 1: Detect nested columns\n",
        "    nested_columns = detect_nested_columns(df)\n",
        "\n",
        "    # Step 2: Normalize if there are nested columns\n",
        "    if nested_columns:\n",
        "        df = normalize_nested_columns(df, nested_columns)\n",
        "        print(f\"{name} has been normalized.\\n\")\n",
        "\n",
        "    # Update the DataFrame in the dictionary (to retain changes outside the loop)\n",
        "    dataframes[name] = df\n",
        "\n",
        "# Access flattened DataFrames as needed\n",
        "business_df = dataframes[\"business_df\"]\n",
        "checkin_df = dataframes[\"checkin_df\"]\n",
        "review_df = dataframes[\"review_df\"]\n",
        "tip_df = dataframes[\"tip_df\"]\n",
        "user_df = dataframes[\"user_df\"]\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-04T02:24:41.513363Z",
          "iopub.execute_input": "2024-11-04T02:24:41.514027Z",
          "iopub.status.idle": "2024-11-04T02:25:08.472041Z",
          "shell.execute_reply.started": "2024-11-04T02:24:41.513971Z",
          "shell.execute_reply": "2024-11-04T02:25:08.470593Z"
        },
        "trusted": true,
        "id": "sOEtEjxDJFQE",
        "outputId": "6df89c6f-8677-493a-991b-dedbaff4e78e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\nProcessing business_df...\nColumn 'attributes' contains nested data.\nColumn 'hours' contains nested data.\nbusiness_df has been normalized.\n\n\nProcessing checkin_df...\nNo nested columns found.\n\nProcessing review_df...\nNo nested columns found.\n\nProcessing tip_df...\nNo nested columns found.\n\nProcessing user_df...\nNo nested columns found.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to handle duplicate column names and fill NaN values\n",
        "def process_dataframe(df):\n",
        "    # Function to deduplicate columns\n",
        "    def deduplicate_columns(columns):\n",
        "        seen = {}\n",
        "        new_columns = []\n",
        "        duplicates_found = False  # Flag to check if there are duplicates\n",
        "\n",
        "        for col in columns:\n",
        "            # If column name already seen, append a counter to make it unique\n",
        "            if col in seen:\n",
        "                duplicates_found = True  # Mark that a duplicate was found\n",
        "                seen[col] += 1\n",
        "                new_columns.append(f\"{col}_{seen[col]}\")\n",
        "            else:\n",
        "                seen[col] = 0\n",
        "                new_columns.append(col)\n",
        "\n",
        "        # Check for duplicates and print appropriate message\n",
        "        if not duplicates_found:\n",
        "            print(\"No duplicate columns found.\")\n",
        "        else:\n",
        "            print(\"Duplicate columns were found and renamed.\")\n",
        "\n",
        "        return new_columns\n",
        "\n",
        "    # Apply deduplication to columns\n",
        "    df.columns = deduplicate_columns(df.columns)\n",
        "\n",
        "    # Fill NaN values with 0\n",
        "    df = df.fillna(0)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Process each DataFrame\n",
        "print(\"Processing business_df:\")\n",
        "business_df = process_dataframe(business_df)\n",
        "\n",
        "print(\"\\nProcessing checkin_df:\")\n",
        "checkin_df = process_dataframe(checkin_df)\n",
        "\n",
        "print(\"\\nProcessing review_df:\")\n",
        "review_df = process_dataframe(review_df)\n",
        "\n",
        "print(\"\\nProcessing tip_df:\")\n",
        "tip_df = process_dataframe(tip_df)\n",
        "\n",
        "print(\"\\nProcessing user_df:\")\n",
        "user_df = process_dataframe(user_df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-04T02:25:29.18535Z",
          "iopub.execute_input": "2024-11-04T02:25:29.185929Z",
          "iopub.status.idle": "2024-11-04T02:25:42.658252Z",
          "shell.execute_reply.started": "2024-11-04T02:25:29.185885Z",
          "shell.execute_reply": "2024-11-04T02:25:42.656683Z"
        },
        "trusted": true,
        "id": "gllZevsWJFQF",
        "outputId": "75ab3aa1-0911-418f-cd2b-04bf0cd97d35"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Processing business_df:\nNo duplicate columns found.\n\nProcessing checkin_df:\nNo duplicate columns found.\n\nProcessing review_df:\nNo duplicate columns found.\n\nProcessing tip_df:\nNo duplicate columns found.\n\nProcessing user_df:\nNo duplicate columns found.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_convert_for_sql(df):\n",
        "    \"\"\"\n",
        "    Cleans and converts data types in a DataFrame for SQL compatibility,\n",
        "    while preserving NaN values as SQL-compatible NULLs. Prints the columns\n",
        "    that were converted.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): The DataFrame to clean and convert.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with SQL-compatible data types.\n",
        "        dict: A dictionary with lists of columns converted to each data type.\n",
        "    \"\"\"\n",
        "    converted_columns = {\n",
        "        \"numeric\": [],\n",
        "        \"text\": [],\n",
        "        \"datetime\": []\n",
        "    }\n",
        "\n",
        "    for col in df.columns:\n",
        "        # Convert numeric columns\n",
        "        if pd.api.types.is_numeric_dtype(df[col]):\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')  # Handles NaN values\n",
        "            converted_columns[\"numeric\"].append(col)\n",
        "\n",
        "        # Convert object columns to strings with length limited to 255 characters\n",
        "        elif pd.api.types.is_object_dtype(df[col]):\n",
        "            df[col] = df[col].astype(str).str.slice(0, 255)  # Convert to string, truncate to 255 characters\n",
        "            df[col] = df[col].replace('nan', pd.NA)  # Replace 'nan' strings with proper NaN values\n",
        "            converted_columns[\"text\"].append(col)\n",
        "\n",
        "        # Convert date columns to datetime\n",
        "        elif pd.api.types.is_datetime64_any_dtype(df[col]) or pd.api.types.is_object_dtype(df[col]):\n",
        "            df[col] = pd.to_datetime(df[col], errors='coerce')  # Convert to datetime, NaN values will be NaT\n",
        "            converted_columns[\"datetime\"].append(col)\n",
        "\n",
        "    # Print converted columns as comma-separated lists\n",
        "    print(\"Converted columns:\")\n",
        "    if converted_columns[\"numeric\"]:\n",
        "        print(\"Numeric Columns:\", \", \".join(converted_columns[\"numeric\"]))\n",
        "    if converted_columns[\"text\"]:\n",
        "        print(\"Text Columns:\", \", \".join(converted_columns[\"text\"]))\n",
        "    if converted_columns[\"datetime\"]:\n",
        "        print(\"Datetime Columns:\", \", \".join(converted_columns[\"datetime\"]))\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply to each DataFrame and display converted columns\n",
        "dataframes = {\n",
        "    \"business_df\": business_df,\n",
        "    \"checkin_df\": checkin_df,\n",
        "    \"review_df\": review_df,\n",
        "    \"tip_df\": tip_df,\n",
        "    \"user_df\": user_df\n",
        "}\n",
        "\n",
        "for name, df in dataframes.items():\n",
        "    print(f\"Cleaning and converting data types for {name}...\")\n",
        "    dataframes[name] = clean_and_convert_for_sql(df)\n",
        "    print(f\"Completed data type conversion for {name}.\\n\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-04T02:25:54.856605Z",
          "iopub.execute_input": "2024-11-04T02:25:54.857154Z",
          "iopub.status.idle": "2024-11-04T02:26:47.19336Z",
          "shell.execute_reply.started": "2024-11-04T02:25:54.857107Z",
          "shell.execute_reply": "2024-11-04T02:26:47.191981Z"
        },
        "trusted": true,
        "id": "crrX3GIPJFQF",
        "outputId": "60cccc6a-fa47-4e6e-c931-b696aa9b9200"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Cleaning and converting data types for business_df...\nConverted columns:\nNumeric Columns: latitude, longitude, stars, review_count, is_open\nText Columns: business_id, name, address, city, state, postal_code, categories, attributes_ByAppointmentOnly, attributes_BusinessAcceptsCreditCards, attributes_BikeParking, attributes_RestaurantsPriceRange2, attributes_CoatCheck, attributes_RestaurantsTakeOut, attributes_RestaurantsDelivery, attributes_Caters, attributes_WiFi, attributes_BusinessParking, attributes_WheelchairAccessible, attributes_HappyHour, attributes_OutdoorSeating, attributes_HasTV, attributes_RestaurantsReservations, attributes_DogsAllowed, attributes_Alcohol, attributes_GoodForKids, attributes_RestaurantsAttire, attributes_Ambience, attributes_RestaurantsTableService, attributes_RestaurantsGoodForGroups, attributes_DriveThru, attributes_NoiseLevel, attributes_GoodForMeal, attributes_BusinessAcceptsBitcoin, attributes_Smoking, attributes_Music, attributes_GoodForDancing, attributes_AcceptsInsurance, attributes_BestNights, attributes_BYOB, attributes_Corkage, attributes_BYOBCorkage, attributes_HairSpecializesIn, attributes_Open24Hours, attributes_RestaurantsCounterService, attributes_AgesAllowed, attributes_DietaryRestrictions, hours_Monday, hours_Tuesday, hours_Wednesday, hours_Thursday, hours_Friday, hours_Saturday, hours_Sunday\nCompleted data type conversion for business_df.\n\nCleaning and converting data types for checkin_df...\nConverted columns:\nText Columns: business_id, date\nCompleted data type conversion for checkin_df.\n\nCleaning and converting data types for review_df...\nConverted columns:\nNumeric Columns: stars, useful, funny, cool\nText Columns: review_id, user_id, business_id, text\nDatetime Columns: date\nCompleted data type conversion for review_df.\n\nCleaning and converting data types for tip_df...\nConverted columns:\nNumeric Columns: compliment_count\nText Columns: user_id, business_id, text\nDatetime Columns: date\nCompleted data type conversion for tip_df.\n\nCleaning and converting data types for user_df...\nConverted columns:\nNumeric Columns: review_count, useful, funny, cool, fans, average_stars, compliment_hot, compliment_more, compliment_profile, compliment_cute, compliment_list, compliment_note, compliment_plain, compliment_cool, compliment_funny, compliment_writer, compliment_photos\nText Columns: user_id, name, yelping_since, elite, friends\nCompleted data type conversion for user_df.\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "business_df.info()\n",
        "checkin_df.info()\n",
        "review_df.info()\n",
        "tip_df.info()\n",
        "user_df.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-04T02:26:47.197946Z",
          "iopub.execute_input": "2024-11-04T02:26:47.198387Z",
          "iopub.status.idle": "2024-11-04T02:26:49.409612Z",
          "shell.execute_reply.started": "2024-11-04T02:26:47.198344Z",
          "shell.execute_reply": "2024-11-04T02:26:49.408234Z"
        },
        "trusted": true,
        "id": "iyDBnL4DJFQG",
        "outputId": "ac7b04d0-2add-4923-f45c-43f9c586fac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 150346 entries, 0 to 150345\nData columns (total 58 columns):\n #   Column                                 Non-Null Count   Dtype  \n---  ------                                 --------------   -----  \n 0   business_id                            150346 non-null  object \n 1   name                                   150346 non-null  object \n 2   address                                150346 non-null  object \n 3   city                                   150346 non-null  object \n 4   state                                  150346 non-null  object \n 5   postal_code                            150346 non-null  object \n 6   latitude                               150346 non-null  float64\n 7   longitude                              150346 non-null  float64\n 8   stars                                  150346 non-null  float64\n 9   review_count                           150346 non-null  int64  \n 10  is_open                                150346 non-null  int64  \n 11  categories                             150346 non-null  object \n 12  attributes_ByAppointmentOnly           150346 non-null  object \n 13  attributes_BusinessAcceptsCreditCards  150346 non-null  object \n 14  attributes_BikeParking                 150346 non-null  object \n 15  attributes_RestaurantsPriceRange2      150346 non-null  object \n 16  attributes_CoatCheck                   150346 non-null  object \n 17  attributes_RestaurantsTakeOut          150346 non-null  object \n 18  attributes_RestaurantsDelivery         150346 non-null  object \n 19  attributes_Caters                      150346 non-null  object \n 20  attributes_WiFi                        150346 non-null  object \n 21  attributes_BusinessParking             150346 non-null  object \n 22  attributes_WheelchairAccessible        150346 non-null  object \n 23  attributes_HappyHour                   150346 non-null  object \n 24  attributes_OutdoorSeating              150346 non-null  object \n 25  attributes_HasTV                       150346 non-null  object \n 26  attributes_RestaurantsReservations     150346 non-null  object \n 27  attributes_DogsAllowed                 150346 non-null  object \n 28  attributes_Alcohol                     150346 non-null  object \n 29  attributes_GoodForKids                 150346 non-null  object \n 30  attributes_RestaurantsAttire           150346 non-null  object \n 31  attributes_Ambience                    150346 non-null  object \n 32  attributes_RestaurantsTableService     150346 non-null  object \n 33  attributes_RestaurantsGoodForGroups    150346 non-null  object \n 34  attributes_DriveThru                   150346 non-null  object \n 35  attributes_NoiseLevel                  150346 non-null  object \n 36  attributes_GoodForMeal                 150346 non-null  object \n 37  attributes_BusinessAcceptsBitcoin      150346 non-null  object \n 38  attributes_Smoking                     150346 non-null  object \n 39  attributes_Music                       150346 non-null  object \n 40  attributes_GoodForDancing              150346 non-null  object \n 41  attributes_AcceptsInsurance            150346 non-null  object \n 42  attributes_BestNights                  150346 non-null  object \n 43  attributes_BYOB                        150346 non-null  object \n 44  attributes_Corkage                     150346 non-null  object \n 45  attributes_BYOBCorkage                 150346 non-null  object \n 46  attributes_HairSpecializesIn           150346 non-null  object \n 47  attributes_Open24Hours                 150346 non-null  object \n 48  attributes_RestaurantsCounterService   150346 non-null  object \n 49  attributes_AgesAllowed                 150346 non-null  object \n 50  attributes_DietaryRestrictions         150346 non-null  object \n 51  hours_Monday                           150346 non-null  object \n 52  hours_Tuesday                          150346 non-null  object \n 53  hours_Wednesday                        150346 non-null  object \n 54  hours_Thursday                         150346 non-null  object \n 55  hours_Friday                           150346 non-null  object \n 56  hours_Saturday                         150346 non-null  object \n 57  hours_Sunday                           150346 non-null  object \ndtypes: float64(3), int64(2), object(53)\nmemory usage: 66.5+ MB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 131930 entries, 0 to 131929\nData columns (total 2 columns):\n #   Column       Non-Null Count   Dtype \n---  ------       --------------   ----- \n 0   business_id  131930 non-null  object\n 1   date         131930 non-null  object\ndtypes: object(2)\nmemory usage: 2.0+ MB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 6990280 entries, 0 to 6990279\nData columns (total 9 columns):\n #   Column       Dtype         \n---  ------       -----         \n 0   review_id    object        \n 1   user_id      object        \n 2   business_id  object        \n 3   stars        int64         \n 4   useful       int64         \n 5   funny        int64         \n 6   cool         int64         \n 7   text         object        \n 8   date         datetime64[ns]\ndtypes: datetime64[ns](1), int64(4), object(4)\nmemory usage: 480.0+ MB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 908915 entries, 0 to 908914\nData columns (total 5 columns):\n #   Column            Non-Null Count   Dtype         \n---  ------            --------------   -----         \n 0   user_id           908915 non-null  object        \n 1   business_id       908915 non-null  object        \n 2   text              908915 non-null  object        \n 3   date              908915 non-null  datetime64[ns]\n 4   compliment_count  908915 non-null  int64         \ndtypes: datetime64[ns](1), int64(1), object(3)\nmemory usage: 34.7+ MB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1987897 entries, 0 to 1987896\nData columns (total 22 columns):\n #   Column              Dtype  \n---  ------              -----  \n 0   user_id             object \n 1   name                object \n 2   review_count        int64  \n 3   yelping_since       object \n 4   useful              int64  \n 5   funny               int64  \n 6   cool                int64  \n 7   elite               object \n 8   friends             object \n 9   fans                int64  \n 10  average_stars       float64\n 11  compliment_hot      int64  \n 12  compliment_more     int64  \n 13  compliment_profile  int64  \n 14  compliment_cute     int64  \n 15  compliment_list     int64  \n 16  compliment_note     int64  \n 17  compliment_plain    int64  \n 18  compliment_cool     int64  \n 19  compliment_funny    int64  \n 20  compliment_writer   int64  \n 21  compliment_photos   int64  \ndtypes: float64(1), int64(16), object(5)\nmemory usage: 333.7+ MB\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to add a primary key column if none exists\n",
        "def ensure_primary_key(df, primary_key_column):\n",
        "    \"\"\"\n",
        "    Checks if the DataFrame has a primary key column and adds one if it doesn't exist.\n",
        "    \"\"\"\n",
        "    if primary_key_column in df.columns:\n",
        "        # Ensure no nulls or duplicates in the existing primary key column\n",
        "        if df[primary_key_column].isnull().any() or df[primary_key_column].duplicated().any():\n",
        "            print(f\"Warning: {primary_key_column} in DataFrame contains nulls or duplicates. Generating new primary key.\")\n",
        "            df['id'] = range(1, len(df) + 1)\n",
        "            return 'id'\n",
        "        else:\n",
        "            return primary_key_column  # The existing column can act as the primary key\n",
        "    else:\n",
        "        # If no primary key column exists, add a sequential 'id' column\n",
        "        df['id'] = range(1, len(df) + 1)\n",
        "        return 'id'\n",
        "\n",
        "# Apply the function to each DataFrame with appropriate column names based on the schema diagram\n",
        "\n",
        "# Business table (primary key: business_id)\n",
        "primary_key_business = ensure_primary_key(business_df, 'business_id')\n",
        "\n",
        "# Checkin table (primary key: id)\n",
        "primary_key_checkin = ensure_primary_key(checkin_df, 'business_id')\n",
        "\n",
        "# Review table (primary key: review_id)\n",
        "primary_key_review = ensure_primary_key(review_df, 'review_id')\n",
        "\n",
        "# Tip table (primary key: id)\n",
        "primary_key_tip = ensure_primary_key(tip_df, 'user_id')\n",
        "\n",
        "# User table (primary key: user_id)\n",
        "primary_key_user = ensure_primary_key(user_df, 'user_id')\n",
        "\n",
        "# Print which column is used as primary key for each DataFrame\n",
        "print(f\"Primary key for business_df: {primary_key_business}\")\n",
        "print(f\"Primary key for checkin_df: {primary_key_checkin}\")\n",
        "print(f\"Primary key for review_df: {primary_key_review}\")\n",
        "print(f\"Primary key for tip_df: {primary_key_tip}\")\n",
        "print(f\"Primary key for user_df: {primary_key_user}\")\n",
        "\n",
        "# For each DataFrame, if a new 'id' column was created, the column will be 'id' as the primary key\n",
        "# Otherwise, it will use the specified primary key column in the schema diagram.\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-04T02:26:53.210282Z",
          "iopub.execute_input": "2024-11-04T02:26:53.210816Z",
          "iopub.status.idle": "2024-11-04T02:27:08.667174Z",
          "shell.execute_reply.started": "2024-11-04T02:26:53.210761Z",
          "shell.execute_reply": "2024-11-04T02:27:08.665822Z"
        },
        "trusted": true,
        "id": "nA8GIyYHJFQG",
        "outputId": "4844972e-ce8f-4ba9-c173-db9225ff36d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Warning: user_id in DataFrame contains nulls or duplicates. Generating new primary key.\nPrimary key for business_df: business_id\nPrimary key for checkin_df: business_id\nPrimary key for review_df: review_id\nPrimary key for tip_df: id\nPrimary key for user_df: user_id\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Function to detect a primary key column or add an 'id' column if no primary key is found\n",
        "def detect_or_create_primary_key(df, table_name):\n",
        "    \"\"\"\n",
        "    Identifies the primary key column for a DataFrame or creates a new 'id' column if no unique non-null column exists.\n",
        "\n",
        "    Parameters:\n",
        "    - df (DataFrame): The DataFrame for which to detect or create a primary key.\n",
        "    - table_name (str): The name of the table (used for printing and tracking purposes).\n",
        "\n",
        "    Returns:\n",
        "    - primary_key (str): The column name that will be used as the primary key.\n",
        "    \"\"\"\n",
        "    # Iterate over each column to check for unique, non-null values\n",
        "    for col in df.columns:\n",
        "        if df[col].is_unique and df[col].notnull().all():\n",
        "            print(f\"Column '{col}' in table '{table_name}' can serve as a primary key.\")\n",
        "            return col  # Return the column if it meets primary key requirements\n",
        "\n",
        "    # If no primary key column was found, create a new 'id' column\n",
        "    df['id'] = range(1, len(df) + 1)\n",
        "    print(f\"No unique non-null column found in table '{table_name}'. Created new primary key column 'id'.\")\n",
        "    return 'id'\n",
        "\n",
        "# Example DataFrames (business_df, checkin_df, review_df, tip_df, user_df)\n",
        "# Assuming these are defined\n",
        "\n",
        "# Detect or create primary key for each DataFrame\n",
        "primary_key_business = detect_or_create_primary_key(business_df, 'business')\n",
        "primary_key_checkin = detect_or_create_primary_key(checkin_df, 'check_in')\n",
        "primary_key_review = detect_or_create_primary_key(review_df, 'review')\n",
        "primary_key_tip = detect_or_create_primary_key(tip_df, 'tip')\n",
        "primary_key_user = detect_or_create_primary_key(user_df, 'user')\n",
        "\n",
        "# Print primary key columns for each table\n",
        "print(f\"Primary key for business_df: {primary_key_business}\")\n",
        "print(f\"Primary key for checkin_df: {primary_key_checkin}\")\n",
        "print(f\"Primary key for review_df: {primary_key_review}\")\n",
        "print(f\"Primary key for tip_df: {primary_key_tip}\")\n",
        "print(f\"Primary key for user_df: {primary_key_user}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-04T02:27:19.941355Z",
          "iopub.execute_input": "2024-11-04T02:27:19.941919Z",
          "iopub.status.idle": "2024-11-04T02:27:37.56749Z",
          "shell.execute_reply.started": "2024-11-04T02:27:19.941869Z",
          "shell.execute_reply": "2024-11-04T02:27:37.566216Z"
        },
        "trusted": true,
        "id": "4RRdgA83JFQG",
        "outputId": "3ba87b13-d4df-4466-aa7c-01c0df9457a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Column 'business_id' in table 'business' can serve as a primary key.\nColumn 'business_id' in table 'check_in' can serve as a primary key.\nColumn 'review_id' in table 'review' can serve as a primary key.\nColumn 'id' in table 'tip' can serve as a primary key.\nColumn 'user_id' in table 'user' can serve as a primary key.\nPrimary key for business_df: business_id\nPrimary key for checkin_df: business_id\nPrimary key for review_df: review_id\nPrimary key for tip_df: id\nPrimary key for user_df: user_id\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Establishing connection with Azure"
      ],
      "metadata": {
        "id": "eLeSrJASJFQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install unixODBC\n",
        "!apt-get update\n",
        "!apt-get install -y unixodbc unixodbc-dev\n",
        "\n",
        "# Install pyodbc\n",
        "!pip install pyodbc\n",
        "\n",
        "# Install sqlalchemy\n",
        "!pip install sqlalchemy\n",
        "\n",
        "# Install ODBC Driver 17 for SQL Server\n",
        "!curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -\n",
        "!curl https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/prod.list | tee /etc/apt/sources.list.d/mssql-release.list\n",
        "!apt-get update\n",
        "!ACCEPT_EULA=Y apt-get install -y msodbcsql17"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-04T02:28:00.833444Z",
          "iopub.execute_input": "2024-11-04T02:28:00.833992Z",
          "iopub.status.idle": "2024-11-04T02:29:07.244395Z",
          "shell.execute_reply.started": "2024-11-04T02:28:00.833946Z",
          "shell.execute_reply": "2024-11-04T02:29:07.242455Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "OUNHK2JhJFQH",
        "outputId": "3978e87b-41e3-49d9-8323-b4e10ecb3a48"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Get:1 https://packages.cloud.google.com/apt gcsfuse-focal InRelease [1227 B]\nGet:2 http://security.ubuntu.com/ubuntu focal-security InRelease [128 kB]      \nHit:3 http://archive.ubuntu.com/ubuntu focal InRelease                         \nGet:4 https://packages.cloud.google.com/apt cloud-sdk InRelease [1618 B]       \nGet:5 http://archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]        \nGet:6 https://packages.cloud.google.com/apt gcsfuse-focal/main amd64 Packages [28.6 kB]\nGet:7 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [4081 kB]\nGet:8 https://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [3375 kB]\nGet:9 https://packages.cloud.google.com/apt cloud-sdk/main all Packages [1564 kB]\nGet:10 http://archive.ubuntu.com/ubuntu focal-backports InRelease [128 kB]     \nGet:11 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [33.5 kB]\nGet:12 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [4258 kB]\nGet:13 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [4105 kB]\nGet:14 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [30.9 kB]\nGet:15 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1275 kB]\nGet:16 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1566 kB]\nGet:17 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [4546 kB]\nFetched 25.2 MB in 5s (5417 kB/s)                            \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following additional packages will be installed:\n  autoconf automake autotools-dev file libltdl-dev libmagic-mgc libmagic1\n  libodbc1 libsigsegv2 libtool m4 odbcinst odbcinst1debian2\nSuggested packages:\n  autoconf-archive gnu-standards autoconf-doc gettext libtool-doc libmyodbc\n  odbc-postgresql tdsodbc unixodbc-bin gfortran | fortran95-compiler gcj-jdk\n  m4-doc\nThe following NEW packages will be installed:\n  autoconf automake autotools-dev file libltdl-dev libmagic-mgc libmagic1\n  libodbc1 libsigsegv2 libtool m4 odbcinst odbcinst1debian2 unixodbc\n  unixodbc-dev\n0 upgraded, 15 newly installed, 0 to remove and 58 not upgraded.\nNeed to get 2236 kB of archives.\nAfter this operation, 15.4 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu focal/main amd64 libmagic-mgc amd64 1:5.38-4 [218 kB]\nGet:2 http://archive.ubuntu.com/ubuntu focal/main amd64 libmagic1 amd64 1:5.38-4 [75.9 kB]\nGet:3 http://archive.ubuntu.com/ubuntu focal/main amd64 file amd64 1:5.38-4 [23.3 kB]\nGet:4 http://archive.ubuntu.com/ubuntu focal/main amd64 libsigsegv2 amd64 2.12-2 [13.9 kB]\nGet:5 http://archive.ubuntu.com/ubuntu focal/main amd64 m4 amd64 1.4.18-4 [199 kB]\nGet:6 http://archive.ubuntu.com/ubuntu focal/main amd64 autoconf all 2.69-11.1 [321 kB]\nGet:7 http://archive.ubuntu.com/ubuntu focal/main amd64 autotools-dev all 20180224.1 [39.6 kB]\nGet:8 http://archive.ubuntu.com/ubuntu focal/main amd64 automake all 1:1.16.1-4ubuntu6 [522 kB]\nGet:9 http://archive.ubuntu.com/ubuntu focal/main amd64 libltdl-dev amd64 2.4.6-14 [162 kB]\nGet:10 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libodbc1 amd64 2.3.6-0.1ubuntu0.1 [190 kB]\nGet:11 http://archive.ubuntu.com/ubuntu focal/main amd64 libtool all 2.4.6-14 [161 kB]\nGet:12 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 odbcinst1debian2 amd64 2.3.6-0.1ubuntu0.1 [41.7 kB]\nGet:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 odbcinst amd64 2.3.6-0.1ubuntu0.1 [14.5 kB]\nGet:14 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 unixodbc amd64 2.3.6-0.1ubuntu0.1 [24.2 kB]\nGet:15 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 unixodbc-dev amd64 2.3.6-0.1ubuntu0.1 [230 kB]\nFetched 2236 kB in 2s (970 kB/s)      \nSelecting previously unselected package libmagic-mgc.\n(Reading database ... 115958 files and directories currently installed.)\nPreparing to unpack .../00-libmagic-mgc_1%3a5.38-4_amd64.deb ...\nUnpacking libmagic-mgc (1:5.38-4) ...\nSelecting previously unselected package libmagic1:amd64.\nPreparing to unpack .../01-libmagic1_1%3a5.38-4_amd64.deb ...\nUnpacking libmagic1:amd64 (1:5.38-4) ...\nSelecting previously unselected package file.\nPreparing to unpack .../02-file_1%3a5.38-4_amd64.deb ...\nUnpacking file (1:5.38-4) ...\nSelecting previously unselected package libsigsegv2:amd64.\nPreparing to unpack .../03-libsigsegv2_2.12-2_amd64.deb ...\nUnpacking libsigsegv2:amd64 (2.12-2) ...\nSelecting previously unselected package m4.\nPreparing to unpack .../04-m4_1.4.18-4_amd64.deb ...\nUnpacking m4 (1.4.18-4) ...\nSelecting previously unselected package autoconf.\nPreparing to unpack .../05-autoconf_2.69-11.1_all.deb ...\nUnpacking autoconf (2.69-11.1) ...\nSelecting previously unselected package autotools-dev.\nPreparing to unpack .../06-autotools-dev_20180224.1_all.deb ...\nUnpacking autotools-dev (20180224.1) ...\nSelecting previously unselected package automake.\nPreparing to unpack .../07-automake_1%3a1.16.1-4ubuntu6_all.deb ...\nUnpacking automake (1:1.16.1-4ubuntu6) ...\nSelecting previously unselected package libltdl-dev:amd64.\nPreparing to unpack .../08-libltdl-dev_2.4.6-14_amd64.deb ...\nUnpacking libltdl-dev:amd64 (2.4.6-14) ...\nSelecting previously unselected package libodbc1:amd64.\nPreparing to unpack .../09-libodbc1_2.3.6-0.1ubuntu0.1_amd64.deb ...\nUnpacking libodbc1:amd64 (2.3.6-0.1ubuntu0.1) ...\nSelecting previously unselected package libtool.\nPreparing to unpack .../10-libtool_2.4.6-14_all.deb ...\nUnpacking libtool (2.4.6-14) ...\nSelecting previously unselected package odbcinst1debian2:amd64.\nPreparing to unpack .../11-odbcinst1debian2_2.3.6-0.1ubuntu0.1_amd64.deb ...\nUnpacking odbcinst1debian2:amd64 (2.3.6-0.1ubuntu0.1) ...\nSelecting previously unselected package odbcinst.\nPreparing to unpack .../12-odbcinst_2.3.6-0.1ubuntu0.1_amd64.deb ...\nUnpacking odbcinst (2.3.6-0.1ubuntu0.1) ...\nSelecting previously unselected package unixodbc.\nPreparing to unpack .../13-unixodbc_2.3.6-0.1ubuntu0.1_amd64.deb ...\nUnpacking unixodbc (2.3.6-0.1ubuntu0.1) ...\nSelecting previously unselected package unixodbc-dev:amd64.\nPreparing to unpack .../14-unixodbc-dev_2.3.6-0.1ubuntu0.1_amd64.deb ...\nUnpacking unixodbc-dev:amd64 (2.3.6-0.1ubuntu0.1) ...\nSetting up libmagic-mgc (1:5.38-4) ...\nSetting up libmagic1:amd64 (1:5.38-4) ...\nSetting up file (1:5.38-4) ...\nSetting up autotools-dev (20180224.1) ...\nSetting up libsigsegv2:amd64 (2.12-2) ...\nSetting up libodbc1:amd64 (2.3.6-0.1ubuntu0.1) ...\nSetting up libtool (2.4.6-14) ...\nSetting up m4 (1.4.18-4) ...\nSetting up autoconf (2.69-11.1) ...\nSetting up automake (1:1.16.1-4ubuntu6) ...\nupdate-alternatives: using /usr/bin/automake-1.16 to provide /usr/bin/automake (automake) in auto mode\nupdate-alternatives: warning: skip creation of /usr/share/man/man1/automake.1.gz because associated file /usr/share/man/man1/automake-1.16.1.gz (of link group automake) doesn't exist\nupdate-alternatives: warning: skip creation of /usr/share/man/man1/aclocal.1.gz because associated file /usr/share/man/man1/aclocal-1.16.1.gz (of link group automake) doesn't exist\nSetting up libltdl-dev:amd64 (2.4.6-14) ...\nSetting up odbcinst1debian2:amd64 (2.3.6-0.1ubuntu0.1) ...\nSetting up unixodbc-dev:amd64 (2.3.6-0.1ubuntu0.1) ...\nSetting up odbcinst (2.3.6-0.1ubuntu0.1) ...\nSetting up unixodbc (2.3.6-0.1ubuntu0.1) ...\nProcessing triggers for libc-bin (2.31-0ubuntu9.16) ...\nProcessing triggers for man-db (2.9.1-1) ...\nProcessing triggers for install-info (6.7.0.dfsg.2-5) ...\nCollecting pyodbc\n  Downloading pyodbc-5.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\nDownloading pyodbc-5.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.0/336.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: pyodbc\nSuccessfully installed pyodbc-5.2.0\nRequirement already satisfied: sqlalchemy in /opt/conda/lib/python3.10/site-packages (2.0.30)\nRequirement already satisfied: typing-extensions>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy) (4.12.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy) (3.0.3)\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   983  100   983    0     0  15841      0 --:--:-- --:--:-- --:--:-- 16114\nOK\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100    89  100    89    0     0   1667      0 --:--:-- --:--:-- --:--:--  1679\ndeb [arch=amd64,armhf,arm64] https://packages.microsoft.com/ubuntu/20.04/prod focal main\nGet:1 https://packages.microsoft.com/ubuntu/20.04/prod focal InRelease [3632 B]\nGet:2 https://packages.microsoft.com/ubuntu/20.04/prod focal/main armhf Packages [22.8 kB]\nHit:3 https://packages.cloud.google.com/apt gcsfuse-focal InRelease            \nGet:4 https://packages.microsoft.com/ubuntu/20.04/prod focal/main arm64 Packages [68.1 kB]\nGet:5 https://packages.microsoft.com/ubuntu/20.04/prod focal/main all Packages [2938 B]\nGet:6 https://packages.microsoft.com/ubuntu/20.04/prod focal/main amd64 Packages [319 kB]\nHit:7 http://security.ubuntu.com/ubuntu focal-security InRelease               \nHit:8 http://archive.ubuntu.com/ubuntu focal InRelease              \nHit:9 https://packages.cloud.google.com/apt cloud-sdk InRelease\nHit:10 http://archive.ubuntu.com/ubuntu focal-updates InRelease\nHit:11 http://archive.ubuntu.com/ubuntu focal-backports InRelease\nFetched 417 kB in 1s (321 kB/s)\nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following NEW packages will be installed:\n  msodbcsql17\n0 upgraded, 1 newly installed, 0 to remove and 63 not upgraded.\nNeed to get 746 kB of archives.\nAfter this operation, 0 B of additional disk space will be used.\nGet:1 https://packages.microsoft.com/ubuntu/20.04/prod focal/main amd64 msodbcsql17 amd64 17.10.6.1-1 [746 kB]\nFetched 746 kB in 0s (8051 kB/s)    \nPreconfiguring packages ...\nSelecting previously unselected package msodbcsql17.\n(Reading database ... 116424 files and directories currently installed.)\nPreparing to unpack .../msodbcsql17_17.10.6.1-1_amd64.deb ...\nUnpacking msodbcsql17 (17.10.6.1-1) ...\nSetting up msodbcsql17 (17.10.6.1-1) ...\nodbcinst: Driver installed. Usage count increased to 1. \n    Target directory is /etc\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import create_engine\n",
        "import sqlalchemy\n",
        "\n",
        "\n",
        "# Define connection details\n",
        "username = 'serveradmin'  # Replace with your Azure SQL admin username\n",
        "password = 'Msba2024'        # Replace with your Azure SQL password\n",
        "server = 'shambhavi64.database.windows.net'\n",
        "database = 'MiniProject1'\n",
        "\n",
        "# Connection string for Azure SQL Server\n",
        "connection_string = f\"mssql+pyodbc://{username}:{password}@{server}:1433/{database}?driver=ODBC+Driver+17+for+SQL+Server\"\n",
        "# connection_string = f\"mssql+pyodbc://{username}:{password}@{server}:1433/{database}?driver=ODBC+Driver+17+for+SQL+Server;Connection Timeout=60\"\n",
        "\n",
        "engine = create_engine(connection_string)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-04T02:29:07.248015Z",
          "iopub.execute_input": "2024-11-04T02:29:07.248523Z",
          "iopub.status.idle": "2024-11-04T02:29:08.306034Z",
          "shell.execute_reply.started": "2024-11-04T02:29:07.24847Z",
          "shell.execute_reply": "2024-11-04T02:29:08.304719Z"
        },
        "trusted": true,
        "id": "FtRjd-KvJFQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing the connection with database\n",
        "from sqlalchemy import text\n",
        "with engine.connect() as connection:\n",
        "    result = connection.execute(text(\"SELECT GETDATE();\"))\n",
        "    for row in result:\n",
        "        print(\"Connected! Server date and time:\", row[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-04T02:30:31.84097Z",
          "iopub.execute_input": "2024-11-04T02:30:31.841578Z",
          "iopub.status.idle": "2024-11-04T02:30:35.584689Z",
          "shell.execute_reply.started": "2024-11-04T02:30:31.841515Z",
          "shell.execute_reply": "2024-11-04T02:30:35.583144Z"
        },
        "trusted": true,
        "id": "KKVj7hoHJFQI",
        "outputId": "f437c9f9-99e5-4391-96c6-1de60dc41763"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Connected! Server date and time: 2024-11-04 02:30:35.297000\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import Column, Integer, String, Table, MetaData\n",
        "from sqlalchemy.types import Integer, String\n",
        "\n",
        "# Define metadata and table with primary key constraint\n",
        "metadata = MetaData()\n",
        "check_in_table = Table(\n",
        "    'check_in', metadata,\n",
        "    Column('id', Integer, primary_key=True, autoincrement=True),  # Primary key column\n",
        "    Column('business_id', String(22)),  # Assuming business_id is VARCHAR(22)\n",
        "    Column('date', String),  # Define other columns as needed\n",
        ")\n",
        "\n",
        "# Create the table in the database\n",
        "metadata.create_all(engine)\n",
        "\n",
        "# Upload data without creating a new table structure\n",
        "checkin_df.to_sql(name=\"check_in\", con=engine, if_exists='append', index=False, chunksize=10000)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-04T02:17:22.966118Z",
          "iopub.status.idle": "2024-11-04T02:17:22.966547Z",
          "shell.execute_reply.started": "2024-11-04T02:17:22.966342Z",
          "shell.execute_reply": "2024-11-04T02:17:22.966363Z"
        },
        "trusted": true,
        "id": "1XDC5dcLJFQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "-- Delete primary key\n",
        "ALTER TABLE review\n",
        "DROP CONSTRAINT IF EXISTS PK_review;\n",
        "\n",
        "ALTER TABLE checkin\n",
        "DROP CONSTRAINT IF EXISTS PK_checkin;\n",
        "\n",
        "ALTER TABLE business\n",
        "DROP CONSTRAINT IF EXISTS PK_business;\n",
        "\n",
        "ALTER TABLE tip\n",
        "DROP CONSTRAINT IF EXISTS PK_tip;\n",
        "\n",
        "ALTER TABLE [user]\n",
        "DROP CONSTRAINT IF EXISTS PK_user;\n",
        "\n",
        "\n",
        "-- delete default foreign key\n",
        "ALTER TABLE review\n",
        "DROP CONSTRAINT IF EXISTS FK_review_business;\n",
        "ALTER TABLE review\n",
        "DROP CONSTRAINT IF EXISTS FK_review_user;\n",
        "\n",
        "ALTER TABLE checkin\n",
        "DROP CONSTRAINT IF EXISTS FK_checkin_business;\n",
        "\n",
        "ALTER TABLE tip\n",
        "DROP CONSTRAINT IF EXISTS FK_tip_business;\n",
        "ALTER TABLE tip\n",
        "DROP CONSTRAINT IF EXISTS FK_tip_user;\n",
        "\n",
        "-- Revise the column metadata\n",
        "ALTER TABLE review\n",
        "ALTER COLUMN review_id VARCHAR(22) NOT NULL;\n",
        "ALTER TABLE review\n",
        "ALTER COLUMN business_id VARCHAR(22) NULL;\n",
        "ALTER TABLE review\n",
        "ALTER COLUMN user_id VARCHAR(22) NULL;\n",
        "\n",
        "ALTER TABLE checkin\n",
        "ALTER COLUMN checkin_id INT NOT NULL;\n",
        "ALTER TABLE checkin\n",
        "ALTER COLUMN business_id VARCHAR(22) NULL;\n",
        "\n",
        "ALTER TABLE business\n",
        "ALTER COLUMN business_id VARCHAR(22) NOT NULL;\n",
        "\n",
        "ALTER TABLE tip\n",
        "ALTER COLUMN tip_id INT NOT NULL;\n",
        "ALTER TABLE tip\n",
        "ALTER COLUMN business_id VARCHAR(22) NULL;\n",
        "ALTER TABLE tip\n",
        "ALTER COLUMN user_id VARCHAR(22) NULL;\n",
        "\n",
        "ALTER TABLE [user]\n",
        "ALTER COLUMN user_id VARCHAR(22) NOT NULL;\n",
        "\n",
        "-- Set the primary key\n",
        "ALTER TABLE review\n",
        "ADD CONSTRAINT PK_review PRIMARY KEY (review_id);\n",
        "\n",
        "ALTER TABLE checkin\n",
        "ADD CONSTRAINT PK_checkin PRIMARY KEY (checkin_id);\n",
        "\n",
        "ALTER TABLE business\n",
        "ADD CONSTRAINT PK_business PRIMARY KEY (business_id);\n",
        "\n",
        "ALTER TABLE tip\n",
        "ADD CONSTRAINT PK_tip PRIMARY KEY (tip_id);\n",
        "\n",
        "ALTER TABLE [user]\n",
        "ADD CONSTRAINT PK_user PRIMARY KEY (user_id);\n",
        "\n",
        "-- Set the foreign key\n",
        "-- ALTER TABLE review\n",
        "-- ADD CONSTRAINT FK_review_business FOREIGN KEY (business_id) REFERENCES business(business_id);\n",
        "-- ALTER TABLE review\n",
        "-- ADD CONSTRAINT FK_review_user FOREIGN KEY (user_id) REFERENCES user(user_id);\n",
        "\n",
        "-- ALTER TABLE checkin\n",
        "-- ADD CONSTRAINT FK_checkin_business FOREIGN KEY (business_id) REFERENCES business(business_id);\n",
        "\n",
        "-- ALTER TABLE tip\n",
        "-- ADD CONSTRAINT FK_tip_business FOREIGN KEY (business_id) REFERENCES business(business_id);\n",
        "-- ALTER TABLE review\n",
        "-- ADD CONSTRAINT FK_tip_user FOREIGN KEY (user_id) REFERENCES user(user_id);"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-04T02:17:22.969084Z",
          "iopub.status.idle": "2024-11-04T02:17:22.969671Z",
          "shell.execute_reply.started": "2024-11-04T02:17:22.969374Z",
          "shell.execute_reply": "2024-11-04T02:17:22.969405Z"
        },
        "trusted": true,
        "id": "piUouaElJFQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Business Queries"
      ],
      "metadata": {
        "id": "FBzwp3P_JFQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table_name = \"business\"  # Replace with the table name you want to verify\n",
        "query = f\"SELECT TOP 10 * FROM {table_name};\"\n",
        "\n",
        "# Execute the query and fetch results in a DataFrame\n",
        "try:\n",
        "    with engine.connect() as connection:\n",
        "        # Execute the query and load the result into a DataFrame\n",
        "        result_df = pd.read_sql_query(text(query), con=connection)\n",
        "        print(result_df)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-04T02:17:22.972223Z",
          "iopub.status.idle": "2024-11-04T02:17:22.972826Z",
          "shell.execute_reply.started": "2024-11-04T02:17:22.972512Z",
          "shell.execute_reply": "2024-11-04T02:17:22.972543Z"
        },
        "trusted": true,
        "id": "d9upUB2JJFQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R1UGUFqHYZ_t"
      }
    }
  ]
}